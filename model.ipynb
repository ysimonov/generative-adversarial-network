{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from natsort import natsorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 100\n",
    "n_filters = 32\n",
    "lr_generator = 3e-4\n",
    "lr_discriminator = 2e-4\n",
    "mode_z = 'normal'\n",
    "image_size = (64, 64)\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "\n",
    "current_path = os.path.dirname(os.path.abspath(__name__))\n",
    "data_path = os.path.normpath(os.path.join(current_path, 'data/celeba/img_align_celeba/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebADataset(Dataset):\n",
    "\n",
    "  def __init__(self, root_dir, transform=None):\n",
    "    image_names = os.listdir(root_dir)\n",
    "    self.root_dir = root_dir\n",
    "    self.transform = transform \n",
    "    self.image_names = natsorted(image_names)\n",
    "\n",
    "  def __len__(self): \n",
    "    return len(self.image_names)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    img_path = os.path.join(self.root_dir, self.image_names[idx])\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    if self.transform:\n",
    "      img = self.transform(img)\n",
    "    return img\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size[0]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), inplace=True)\n",
    "])\n",
    "\n",
    "dataset = CelebADataset(root_dir=data_path, transform=transform)\n",
    "dataset_dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=8, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, n_filters):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=input_size,\n",
    "                out_channels=n_filters*4,\n",
    "                kernel_size=4,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(\n",
    "                num_features=n_filters*4,\n",
    "                track_running_stats=False\n",
    "            ),\n",
    "            nn.LeakyReLU(\n",
    "                negative_slope=0.2,\n",
    "                inplace=True\n",
    "            ),\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=n_filters*4,\n",
    "                out_channels=n_filters*2,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(\n",
    "                num_features=n_filters*2,\n",
    "                track_running_stats=False\n",
    "            ),\n",
    "            nn.LeakyReLU(\n",
    "                negative_slope=0.2,\n",
    "                inplace=True\n",
    "            ),\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=n_filters*2,\n",
    "                out_channels=n_filters,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(\n",
    "                num_features=n_filters,\n",
    "                track_running_stats=False\n",
    "            ),\n",
    "            nn.LeakyReLU(\n",
    "                negative_slope=0.2,\n",
    "                inplace=True\n",
    "            ),\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=n_filters,\n",
    "                out_channels=1,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.Tanh()          \n",
    "        )\n",
    "    def forward(self, input):\n",
    "        output = self.network(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): ConvTranspose2d(100, 128, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "  (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (3): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "  (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (6): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "  (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (9): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (10): Tanh()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "generator = Generator(\n",
    "    input_size=input_size,\n",
    "    n_filters=n_filters\n",
    ").to(device=device)\n",
    "\n",
    "print(generator.network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_filters):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=n_filters,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.LeakyReLU(\n",
    "                negative_slope=0.2,\n",
    "                inplace=True\n",
    "            ),\n",
    "            nn.Conv2d(\n",
    "                in_channels=n_filters,\n",
    "                out_channels=n_filters*2,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(\n",
    "                num_features=n_filters*2,\n",
    "                track_running_stats=False\n",
    "            ),\n",
    "            nn.LeakyReLU(\n",
    "                negative_slope=0.2,\n",
    "                inplace=True\n",
    "            ),\n",
    "            nn.Conv2d(\n",
    "                in_channels=n_filters*2,\n",
    "                out_channels=n_filters*4,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(\n",
    "                num_features=n_filters*4,\n",
    "                track_running_stats=False\n",
    "            ),\n",
    "            nn.LeakyReLU(\n",
    "                negative_slope=0.2,\n",
    "                inplace=True\n",
    "            ),\n",
    "            nn.Conv2d(\n",
    "                in_channels=n_filters*4,\n",
    "                out_channels=1,\n",
    "                kernel_size=4,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.Sigmoid()          \n",
    "        )\n",
    "    def forward(self, input):\n",
    "        output = self.network(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "  (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "  (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (8): Conv2d(128, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "  (9): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "discriminator = Discriminator(\n",
    "    n_filters=n_filters\n",
    ").to(device=device)\n",
    "\n",
    "print(discriminator.network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.BCELoss()\n",
    "generator_optimizer = torch.optim.Adam(\n",
    "    params=generator.parameters(), \n",
    "    lr=lr_generator\n",
    ")\n",
    "discriminator_optimizer = torch.optim.Adam(\n",
    "    params=discriminator.parameters(),\n",
    "    lr=lr_discriminator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_noise(batch_size, input_size, mode_z):\n",
    "    if mode_z == 'uniform':\n",
    "        input_z = torch.rand(batch_size, input_size, 1, 1) * 2 - 1\n",
    "    elif mode_z == 'normal':\n",
    "        input_z = torch.rand(batch_size, input_size, 1, 1)\n",
    "    return input_z\n",
    "\n",
    "def train_discriminator(input: torch.Tensor):\n",
    "    discriminator.zero_grad()\n",
    "    batch_size = input.size(0)\n",
    "    input = input.to(device)\n",
    "    discriminator_labels_real = torch.ones(batch_size, 1, device=device)\n",
    "    discriminator_prob_real = discriminator(input)\n",
    "    discriminator_loss_real = loss_func(discriminator_prob_real, discriminator_labels_real)\n",
    "    input_z = create_noise(batch_size, input_size, mode_z)\n",
    "    generator_output = generator(input_z)\n",
    "    discriminator_prob_fake = discriminator(generator_output)\n",
    "    discriminator_labels_fake = torch.zeros(batch_size, 1, device=device)\n",
    "    discriminator_loss_fake = loss_func(discriminator_prob_fake, discriminator_labels_fake)\n",
    "    discriminator_loss = discriminator_loss_real + discriminator_loss_fake\n",
    "    discriminator_loss.backward()\n",
    "    discriminator_optimizer.step()\n",
    "    return (\n",
    "        discriminator_loss.data.item(),\n",
    "        discriminator_prob_real.detach(),\n",
    "        discriminator_prob_fake.detach()\n",
    "    )\n",
    "\n",
    "def train_generator(input: torch.Tensor):\n",
    "    generator.zero_grad()\n",
    "    batch_size = input.size(0)\n",
    "    input_z = create_noise(batch_size, input_size, mode_z).to(device)\n",
    "    generator_labels_real = torch.ones(batch_size, 1, device=device)\n",
    "    generator_output = generator(input_z)\n",
    "    discriminator_prob_fake = discriminator(generator_output)\n",
    "    generator_loss = loss_func(discriminator_prob_fake, generator_labels_real)\n",
    "    generator_loss.backward()\n",
    "    generator_optimizer.step()\n",
    "    return generator_loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_samples(generator, input_z):\n",
    "    generator_output = generator(input_z)\n",
    "    images = torch.reshape(generator_output, (batch_size, *image_size))\n",
    "    return (images + 1) / 2.0\n",
    "\n",
    "\n",
    "fixed_z = create_noise(batch_size, input_size, mode_z).to(device)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "discriminator_losses = []\n",
    "generator_losses = []\n",
    "epoch_samples = []\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    generator.train()\n",
    "    for i, (input, _) in enumerate(dataset_dataloader):\n",
    "        discriminator_loss, discriminator_prob_real, discriminator_prob_fake = train_discriminator(input)\n",
    "        discriminator_losses.append(discriminator_loss)\n",
    "        generator_loss = train_generator(input)\n",
    "        generator_losses.append(generator_loss)\n",
    "    print(  f'Epoch {epoch:03d} | Avg Losses >>'\n",
    "            f' G/D {torch.FloatTensor(generator_losses).mean():.4f}'\n",
    "            f'/{torch.FloatTensor(discriminator_losses).mean():.4f}')\n",
    "    generator.eval()\n",
    "    epoch_samples.append(\n",
    "        create_samples(\n",
    "            generator, fixed_z\n",
    "        ).detach().cpu().numpy()\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d12e26f72a40d5c48233c54861fca038d6a92135fc0e5e920944e69c69b6b29d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
